# Simple AVSR-LLM configuration

# Model configuration
model:
  # LLM configuration
  llm_path: "checkpoints/Llama-3.2-1B"  # Path to LLM model
  
  # Audio encoder configuration
  whisper_model: "openai/whisper-medium"  # Path to Whisper model
  
  # Video encoder configuration
  clip_model: "openai/clip-vit-base-patch32"  # Path to CLIP model
  
  # Use FP16 for better performance
  fp16: true

# Data configuration
data:
  # Data path
  path: "/home/rishabh/Desktop/Datasets/lrs3/433h_data"  # Path to dataset
  
  # Training data
  train_manifest: "train.tsv"  # Training data manifest
  train_labels: "train.wrd"    # Training data labels
  
  # Validation data
  val_manifest: "valid.tsv"    # Validation data manifest
  val_labels: "valid.wrd"      # Validation data labels
  
  # Data processing
  max_audio_length: 30         # Maximum audio length in seconds
  max_video_length: 300        # Maximum video length in frames
  
  # DataLoader configuration
  batch_size: 4                # Batch size
  num_workers: 4               # Number of workers for dataloader

# Training configuration
training:
  # Optimization
  learning_rate: 5e-5          # Learning rate
  weight_decay: 0.01           # Weight decay
  max_epochs: 10               # Maximum number of epochs
  grad_accum_steps: 4          # Gradient accumulation steps
  
  # Logging and checkpointing
  log_interval: 10             # Log every N batches
  save_interval: 1             # Save checkpoint every N epochs 