^C
cd /home/rishabh/Desktop/Experiments/AVSR-LLM && python3 -c "f = open('scripts/train.py', 'w'); f.write('#!/usr/bin/env python3\\n\\n\"\"\"\\nTrain AVSR-LLM model\\n\"\"\"\\n\\nimport os\\nimport sys\\nimport argparse\\nimport torch\\nimport numpy as np\\nimport random\\n\\n# Add parent directory to path to allow importing from src\\nscript_dir = os.path.dirname(os.path.abspath(__file__))\\nparent_dir = os.path.dirname(script_dir)\\nsys.path.insert(0, parent_dir)\\n\\nfrom src.models.avsr_llm import AVSRLLM\\nfrom src.data.dataset import AVSRDataset, create_dataloader\\nfrom src.train.trainer import Trainer, create_optimizer, create_scheduler\\n\\n\\ndef parse_args():\\n    \"\"\"Parse command line arguments\"\"\"\\n    parser = argparse.ArgumentParser(description=\"Train AVSR-LLM model\")\\n    \\n    # Model arguments\\n    parser.add_argument(\"--avhubert_path\", type=str, required=True,\\n                        help=\"Path to AV-HuBERT checkpoint\")\\n    parser.add_argument(\"--llm_path\", type=str, required=True,\\n                        help=\"Path to LLM model\")\\n    parser.add_argument(\"--use_audio\", action=\"store_true\",\\n                        help=\"Use audio modality\")\\n    parser.add_argument(\"--use_video\", action=\"store_true\",\\n                        help=\"Use video modality\")\\n    parser.add_argument(\"--freeze_encoder\", action=\"store_true\",\\n                        help=\"Freeze encoder parameters\")\\n    parser.add_argument(\"--encoder_finetune_layers\", type=int, nargs=\"*\",\\n                        help=\"List of encoder layers to finetune\")\\n    parser.add_argument(\"--encoder_layer\", type=int, default=-1,\\n                        help=\"Encoder layer to extract features from\")\\n    \\n    # LLM arguments\\n    parser.add_argument(\"--use_lora\", action=\"store_true\",\\n                        help=\"Use LoRA for fine-tuning\")\\n    parser.add_argument(\"--use_8bit\", action=\"store_true\",\\n                        help=\"Use 8-bit quantization\")\\n    parser.add_argument(\"--lora_r\", type=int, default=16,\\n                        help=\"LoRA rank\")\\n    parser.add_argument(\"--lora_alpha\", type=int, default=32,\\n                        help=\"LoRA alpha\")\\n    parser.add_argument(\"--lora_dropout\", type=float, default=0.1,\\n                        help=\"LoRA dropout\")\\n    parser.add_argument(\"--prompt_template\", type=str, default=\"Transcribe the speech: \",\\n                        help=\"Prompt template for LLM\")\\n    \\n    # Data arguments\\n    parser.add_argument(\"--train_manifest\", type=str, required=True,\\n                        help=\"Path to training manifest file\")\\n    parser.add_argument(\"--train_labels\", type=str, required=True,\\n                        help=\"Path to training labels file\")\\n    parser.add_argument(\"--val_manifest\", type=str,\\n                        help=\"Path to validation manifest file\")\\n    parser.add_argument(\"--val_labels\", type=str,\\n                        help=\"Path to validation labels file\")\\n    parser.add_argument(\"--data_root\", type=str,\\n                        help=\"Root directory for data paths\")\\n    parser.add_argument(\"--max_audio_length\", type=int, default=480000,\\n                        help=\"Maximum audio length in samples\")\\n    parser.add_argument(\"--max_video_length\", type=int, default=600,\\n                        help=\"Maximum video length in frames\")\\n    \\n    # Training arguments\\n    parser.add_argument(\"--output_dir\", type=str, default=\"./output\",\\n                        help=\"Output directory\")\\n    parser.add_argument(\"--batch_size\", type=int, default=4,\\n                        help=\"Batch size\")\\n    parser.add_argument(\"--grad_accum_steps\", type=int, default=4,\\n                        help=\"Gradient accumulation steps\")\\n    parser.add_argument(\"--max_epochs\", type=int, default=10,\\n                        help=\"Maximum number of epochs\")\\n    parser.add_argument(\"--learning_rate\", type=float, default=5e-5,\\n                        help=\"Learning rate\")\\n    parser.add_argument(\"--weight_decay\", type=float, default=0.01,\\n                        help=\"Weight decay\")\\n    parser.add_argument(\"--warmup_ratio\", type=float, default=0.1,\\n                        help=\"Warmup ratio\")\\n    parser.add_argument(\"--clip_grad_norm\", type=float, default=1.0,\\n                        help=\"Gradient clipping norm\")\\n    parser.add_argument(\"--seed\", type=int, default=42,\\n                        help=\"Random seed\")\\n    parser.add_argument(\"--num_workers\", type=int, default=4,\\n                        help=\"Number of data loader workers\")\\n    parser.add_argument(\"--log_interval\", type=int, default=10,\\n                        help=\"Log interval\")\\n    parser.add_argument(\"--save_interval\", type=int, default=1,\\n                        help=\"Save interval (epochs)\")\\n    parser.add_argument(\"--eval_steps\", type=int,\\n                        help=\"Evaluate every N steps\")\\n    parser.add_argument(\"--resume_from\", type=str,\\n                        help=\"Resume from checkpoint\")\\n    \\n    return parser.parse_args()\\n\\n\\ndef main():\\n    \"\"\"Main training function\"\"\"\\n    # Parse arguments\\n    args = parse_args()\\n    \\n    # Set random seed\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    torch.manual_seed(args.seed)\\n    if torch.cuda.is_available():\\n        torch.cuda.manual_seed_all(args.seed)\\n    \\n    # Create output directory\\n    os.makedirs(args.output_dir, exist_ok=True)\\n    \\n    # Save arguments\\n    import json\\n    with open(os.path.join(args.output_dir, \"args.json\"), \"w\") as f:\\n        json.dump(vars(args), f, indent=4)\\n    \\n    # Determine modalities to use\\n    modalities = []\\n    if args.use_audio:\\n        modalities.append(\"audio\")\\n    if args.use_video:\\n        modalities.append(\"video\")\\n    \\n    if not modalities:\\n        print(\"Error: At least one modality (audio or video) must be selected\")\\n        sys.exit(1)\\n    \\n    print(f\"Using modalities: {modalities}\")\\n    \\n    # Create datasets\\n    print(f\"Loading training data from {args.train_manifest}\")\\n    train_dataset = AVSRDataset(\\n        manifest_path=args.train_manifest,\\n        label_path=args.train_labels,\\n        root_dir=args.data_root,\\n        modalities=modalities,\\n        max_audio_length=args.max_audio_length,\\n        max_video_length=args.max_video_length,\\n        split=\"train\"\\n    )\\n    \\n    val_dataset = None\\n    if args.val_manifest and args.val_labels:\\n        print(f\"Loading validation data from {args.val_manifest}\")\\n        val_dataset = AVSRDataset(\\n            manifest_path=args.val_manifest,\\n            label_path=args.val_labels,\\n            root_dir=args.data_root,\\n            modalities=modalities,\\n            max_audio_length=args.max_audio_length,\\n            max_video_length=args.max_video_length,\\n            split=\"val\"\\n        )\\n    \\n    # Create data loaders\\n    train_dataloader = create_dataloader(\\n        train_dataset,\\n        batch_size=args.batch_size,\\n        shuffle=True,\\n        num_workers=args.num_workers\\n    )\\n    \\n    val_dataloader = None\\n    if val_dataset is not None:\\n        val_dataloader = create_dataloader(\\n            val_dataset,\\n            batch_size=args.batch_size,\\n            shuffle=False,\\n            num_workers=args.num_workers\\n        )\\n    \\n    # Create model\\n    print(\"Initializing model\")\\n    model = AVSRLLM(\\n        avhubert_path=args.avhubert_path,\\n        llm_path=args.llm_path,\\n        use_audio=args.use_audio,\\n        use_video=args.use_video,\\n        freeze_encoder=args.freeze_encoder,\\n        encoder_finetune_layers=args.encoder_finetune_layers,\\n        encoder_layer=args.encoder_layer,\\n        use_lora=args.use_lora,\\n        use_8bit=args.use_8bit,\\n        lora_r=args.lora_r,\\n        lora_alpha=args.lora_alpha,\\n        lora_dropout=args.lora_dropout,\\n        prompt_template=args.prompt_template,\\n    )\\n    \\n    # Create optimizer\\n    optimizer = create_optimizer(\\n        model,\\n        learning_rate=args.learning_rate,\\n        weight_decay=args.weight_decay\\n    )\\n    \\n    # Create scheduler\\n    total_steps = len(train_dataloader) // args.grad_accum_steps * args.max_epochs\\n    scheduler = create_scheduler(\\n        optimizer,\\n        num_training_steps=total_steps,\\n        warmup_ratio=args.warmup_ratio\\n    )\\n    \\n    # Create trainer\\n    trainer = Trainer(\\n        model=model,\\n        train_dataloader=train_dataloader,\\n        val_dataloader=val_dataloader,\\n        optimizer=optimizer,\\n        scheduler=scheduler,\\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\\n        output_dir=args.output_dir,\\n        max_epochs=args.max_epochs,\\n        log_interval=args.log_interval,\\n        save_interval=args.save_interval,\\n        grad_accum_steps=args.grad_accum_steps,\\n        clip_grad_norm=args.clip_grad_norm,\\n        eval_steps=args.eval_steps,\\n    )\\n    \\n    # Resume from checkpoint if specified\\n    if args.resume_from:\\n        trainer.load_checkpoint(args.resume_from)\\n    \\n    # Train model\\n    print(f\"Starting training for {args.max_epochs} epochs\")\\n    trainer.train()\\n    \\n    print(f\"Training complete. Model saved to {args.output_dir}\")\\n\\n\\nif __name__ == \"__main__\":\\n    main()'); f.close()"
 
